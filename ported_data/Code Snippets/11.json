{"code_in_html": "import numpy\n_multi=False\n_ncpus=1\n\ntry:\n  # May raise ImportError\n  import multiprocessing\n  _multi=True\n\n  # May raise NotImplementedError\n  _ncpus = multiprocessing.cpu_count()\nexcept:\n  pass\n\n\n__all__ = ('parallel_map',)\n\n\ndef worker(f, ii, chunk, out_q, err_q, lock):\n  \"\"\"\n  A worker function that maps an input function over a\n  slice of the input iterable.\n\n  :param f  : callable function that accepts argument from iterable\n  :param ii  : process ID\n  :param chunk: slice of input iterable\n  :param out_q: thread-safe output queue\n  :param err_q: thread-safe queue to populate on exception\n  :param lock : thread-safe lock to protect a resource\n         ( useful in extending parallel_map() )\n  \"\"\"\n  vals = []\n\n  # iterate over slice \n  for val in chunk:\n    try:\n      result = f(val)\n    except Exception, e:\n      err_q.put(e)\n      return\n\n    vals.append(result)\n\n  # output the result and task ID to output queue\n  out_q.put( (ii, vals) )\n\n\ndef run_tasks(procs, err_q, out_q, num):\n  \"\"\"\n  A function that executes populated processes and processes\n  the resultant array. Checks error queue for any exceptions.\n\n  :param procs: list of Process objects\n  :param out_q: thread-safe output queue\n  :param err_q: thread-safe queue to populate on exception\n  :param num : length of resultant array\n\n  \"\"\"\n  # function to terminate processes that are still running.\n  die = (lambda vals : [val.terminate() for val in vals\n             if val.exitcode is None])\n\n  try:\n    for proc in procs:\n      proc.start()\n\n    for proc in procs:\n      proc.join()\n\n  except Exception, e:\n    # kill all slave processes on ctrl-C\n    die(procs)\n    raise e\n\n  if not err_q.empty():\n    # kill all on any exception from any one slave\n    die(procs)\n    raise err_q.get()\n\n  # Processes finish in arbitrary order. Process IDs double\n  # as index in the resultant array.\n  results=[None]*num;\n  while not out_q.empty():\n    idx, result = out_q.get()\n    results[idx] = result\n\n  # Remove extra dimension added by array_split\n  return list(numpy.concatenate(results))\n\n\ndef parallel_map(function, sequence, numcores=None):\n  \"\"\"\n  A parallelized version of the native Python map function that\n  utilizes the Python multiprocessing module to divide and \n  conquer sequence.\n\n  parallel_map does not yet support multiple argument sequences.\n\n  :param function: callable function that accepts argument from iterable\n  :param sequence: iterable sequence \n  :param numcores: number of cores to use\n  \"\"\"\n  if not callable(function):\n    raise TypeError(\"input function '%s' is not callable\" %\n              repr(function))\n\n  if not numpy.iterable(sequence):\n    raise TypeError(\"input '%s' is not iterable\" %\n              repr(sequence))\n\n  size = len(sequence)\n\n  if not _multi or size == 1:\n    return map(function, sequence)\n\n  if numcores is None:\n    numcores = _ncpus\n\n  # Returns a started SyncManager object which can be used for sharing \n  # objects between processes. The returned manager object corresponds\n  # to a spawned child process and has methods which will create shared\n  # objects and return corresponding proxies.\n  manager = multiprocessing.Manager()\n\n  # Create FIFO queue and lock shared objects and return proxies to them.\n  # The managers handles a server process that manages shared objects that\n  # each slave process has access to. Bottom line -- thread-safe.\n  out_q = manager.Queue()\n  err_q = manager.Queue()\n  lock = manager.Lock()\n\n  # if sequence is less than numcores, only use len sequence number of \n  # processes\n  if size < numcores:\n    numcores = size \n\n  # group sequence into numcores-worth of chunks\n  sequence = numpy.array_split(sequence, numcores)\n\n  procs = [multiprocessing.Process(target=worker,\n           args=(function, ii, chunk, out_q, err_q, lock))\n         for ii, chunk in enumerate(sequence)]\n\n  return run_tasks(procs, err_q, out_q, numcores)\n\n\nif __name__ == \"__main__\":\n  \"\"\"\n  Unit test of parallel_map()\n\n  Create an arbitrary length list of references to a single\n  matrix containing random floats and compute the eigenvals\n  in serial and parallel. Compare the results and timings.\n  \"\"\"\n\n  import time\n\n  numtasks = 5\n  #size = (1024,1024)\n  size = (512,512)\n\n  vals = numpy.random.rand(*size)\n  f = numpy.linalg.eigvals\n\n  iterable = [vals]*numtasks\n\n  print ('Running numpy.linalg.eigvals %iX on matrix size [%i,%i]' %\n      (numtasks,size[0],size[1]))\n\n  tt = time.time()\n  presult = parallel_map(f, iterable)\n  print 'parallel map in %g secs' % (time.time()-tt)\n\n  tt = time.time()\n  result = map(f, iterable)\n  print 'serial map in %g secs' % (time.time()-tt)\n\n  assert (numpy.asarray(result) == numpy.asarray(presult)).all()\n\n\n", "description": "[<p></p>]", "author": "aldcroft ", "tags": ["distributed", "numpy"], "date_published": "2010-03-16", "title": "Parallel map using multiprocessing"}